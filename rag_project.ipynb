{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API Keys Configuration\n",
        "YOUR_GOOGLE_API_KEY = \"\"\n",
        "YOUR_LANGCHAIN_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjFOYDtg8WcI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import MessagesPlaceholder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imYBELBm6YS-"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "l9_ioQML6yKM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using Gemini embeddings\n"
          ]
        }
      ],
      "source": [
        "USE_LOCAL_EMBEDDINGS = False  # set to False if you want Gemini embeddings\n",
        "\n",
        "if USE_LOCAL_EMBEDDINGS:\n",
        "    from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    print(\"✅ Using local embeddings (all-MiniLM-L6-v2)\")\n",
        "else:\n",
        "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    print(\"✅ Using Gemini embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "v0J3NFkv6yO2"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\", convert_system_message_to_human=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OjwBo6v8SKO",
        "outputId": "06843182-1367-4da0-a94a-8f8882b675c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi! RAG is a really important and popular technique for making Large Language Models (LLMs) like ChatGPT even more powerful and reliable.\n",
            "\n",
            "RAG stands for **Retrieval-Augmented Generation**.\n",
            "\n",
            "Let's break down what that means and why it's so useful:\n",
            "\n",
            "---\n",
            "\n",
            "### What Problem Does RAG Solve?\n",
            "\n",
            "LLMs are incredibly good at generating human-like text, but they have a few limitations:\n",
            "\n",
            "1.  **Knowledge Cutoff:** Their knowledge is limited to the data they were trained on. If something happened after their last training update, they won't know about it.\n",
            "2.  **Hallucinations:** Sometimes, if an LLM doesn't know the answer, it might confidently \"make up\" information that sounds plausible but is factually incorrect.\n",
            "3.  **Lack of Specificity:** They might not have deep, specialized knowledge about a very specific domain (e.g., your company's internal policies, a niche scientific field).\n",
            "4.  **No Source Citation:** They can't tell you *where* they got their information from, making it hard to verify.\n",
            "\n",
            "RAG addresses these issues by giving the LLM a way to look up and use **external, up-to-date, and factual information** *before* it generates a response.\n",
            "\n",
            "---\n",
            "\n",
            "### How Does RAG Work? (The Analogy)\n",
            "\n",
            "Imagine you're asking a very smart person (the LLM) a question.\n",
            "\n",
            "*   **Without RAG:** They answer purely from their memory. If they don't know, they might guess or say \"I don't know.\"\n",
            "*   **With RAG:** Before they answer, you hand them a relevant book, document, or a collection of articles and say, \"Here, read this first, then answer my question based on what's in here.\" They then give you a well-researched answer, perhaps even citing pages.\n",
            "\n",
            "---\n",
            "\n",
            "### The Steps of RAG in Detail:\n",
            "\n",
            "1.  **Retrieval (The 'R' part):**\n",
            "    *   When you ask a question, the RAG system first takes your question and uses it to search through a designated \"knowledge base\" (which could be a collection of your company documents, web pages, a database, etc.).\n",
            "    *   It uses smart search techniques (often involving vector embeddings and similarity search) to find the most relevant pieces of information or documents related to your query.\n",
            "    *   **Example:** If you ask \"What are the Q3 sales figures for Acme Corp?\", the system might retrieve the latest Q3 financial report for Acme Corp.\n",
            "\n",
            "2.  **Augmentation:**\n",
            "    *   The retrieved pieces of information are then added to your original question.\n",
            "    *   This creates an \"augmented prompt\" or \"context\" that is much richer than your initial simple question.\n",
            "    *   **Example:** The prompt sent to the LLM becomes something like: \"Here is some information about Acme Corp's Q3: [retrieved text from financial report]. Now, based on this information, what are the Q3 sales figures for Acme Corp?\"\n",
            "\n",
            "3.  **Generation (The 'G' part):**\n",
            "    *   The LLM then receives *this augmented prompt*.\n",
            "    *   It uses the provided context (the retrieved information) to formulate its answer. This forces the LLM to \"ground\" its response in the factual data you've provided, rather than relying solely on its internal, potentially outdated, or generic training.\n",
            "    *   **Result:** A more accurate, relevant, and verifiable answer.\n",
            "\n",
            "---\n",
            "\n",
            "### Key Benefits of RAG:\n",
            "\n",
            "*   **Improved Accuracy and Factuality:** Significantly reduces hallucinations.\n",
            "*   **Up-to-Date Information:** Can access the very latest data from your knowledge base.\n",
            "*   **Domain-Specific Knowledge:** Tailor LLMs to specific industries, companies, or topics without retraining the entire model.\n",
            "*   **Transparency and Trust:** Can often provide sources for its answers, increasing user trust.\n",
            "*   **Cost-Effective:** No need to constantly retrain the entire LLM with new data; just update your knowledge base.\n",
            "*   **Enhanced Control:** You control the information the LLM uses, ensuring it adheres to specific guidelines or data.\n",
            "\n",
            "---\n",
            "\n",
            "### Real-World Examples:\n",
            "\n",
            "*   **Customer Service Chatbots:** A RAG-powered chatbot can answer specific questions about a product using up-to-date manuals, FAQs, and support documents.\n",
            "*   **Internal Knowledge Bases:** Employees can ask an LLM questions about company policies, HR benefits, or project documentation, and get accurate answers pulled directly from internal sources.\n",
            "*   **Legal/Medical Research:** LLMs can summarize complex legal cases or medical journals, providing answers grounded in specific texts.\n",
            "*   **Personalized Search:** Instead of just showing links, a RAG system could summarize search results by pulling information from the top articles.\n",
            "\n",
            "In essence, RAG makes LLMs far more reliable and useful for tasks requiring specific, current, and verifiable information by giving them a powerful \"open book\" capability.\n"
          ]
        }
      ],
      "source": [
        "print(model.invoke(\"hi, what is RAG?\").content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "10gWvLAPgog5"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "_30WDYVNhNcB"
      },
      "outputs": [],
      "source": [
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWhwa8EehPBY",
        "outputId": "abfcae78-d1fd-4f83-ed80-78ab5ef98555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 document(s)\n",
            "First document preview: \n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a ...\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loaded {len(doc)} document(s)\")\n",
        "print(f\"First document preview: {doc[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "5OACjayEh_-Z"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gK5tAF0AiToT"
      },
      "outputs": [],
      "source": [
        "splits = text_splitter.split_documents(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "JITEEuC2iWaJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 63 chunks\n",
            "First chunk preview: LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool con...\n"
          ]
        }
      ],
      "source": [
        "print(f\"Split into {len(splits)} chunks\")\n",
        "print(f\"First chunk preview: {splits[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZIid8B0jMZr",
        "outputId": "2cedd121-905f-4d13-c180-78f883592be0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000013DE8832650>, search_kwargs={})"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "cdv3A3srjPZK"
      },
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant for question answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer the question \"\n",
        "    \"If you don't know the answer, say that you don't know.\"\n",
        "    \"Use three sentences maximum and keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Dk5_nIYGj6T4"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lklYQhQkDpe"
      },
      "outputs": [],
      "source": [
        "question_answering_chain = create_stuff_documents_chain(model, chat_prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answering_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWgW4_9zkflk",
        "outputId": "5df2b2de-1c91-4b9c-8f00-f9563b8833e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'What is MRKL?',\n",
              " 'context': [Document(id='ca0ad523-4120-4f77-8998-a04529dee03b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
              "  Document(id='e4bba22e-3af4-4c21-aeed-363aa516c882', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
              "  Document(id='5678f45f-158d-43ac-a2f0-d97074c8fac4', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
              "  Document(id='10b41364-f27b-46c9-aa36-4afd8fb457ae', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:')],\n",
              " 'answer': 'MRKL, short for “Modular Reasoning, Knowledge and Language,” is a neuro-symbolic architecture designed for autonomous agents. It comprises a collection of “expert” modules, with a general-purpose LLM acting as a router to direct inquiries to the most suitable module. These modules can be either neural, like deep learning models, or symbolic, such as a math calculator or a weather API.'}"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"input\": \"What is MRKL?\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
